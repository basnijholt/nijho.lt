---
title: "ðŸ“± Agentic Phone Coding: WireGuard, Blink Shell, and My Self-Hosted AI Stack"
subtitle: "A reproducible voice-driven workflow using Mosh, Zellij, FasterWhisper, Ollama, and my agent-cli server to build software from an iPhone without sending code to SaaS."
summary: "My mostly self-hosted mobile development loop: WireGuard from an iPhone into my NixOS machine, persistent Blink/Mosh sessions, a systemd-managed agent-cli server, and an iOS Shortcut for FasterWhisper+Ollama dictationâ€”paired with the best proprietary coding model available."
date: 2025-10-20
draft: false
featured: false
authors:
  - admin
tags:
  - agentic-coding
  - ai
  - workflow
  - ios
  - self-hosting
  - nixos
  - wireguard
  - blink
  - zellij
  - ollama
  - faster-whisper
  - agent-cli
categories:
  - Software Development
  - AI
  - level:intermediate
image:
  caption: ""
  focal_point: "Smart"
  placement: 2
  preview_only: false
---

I am dictating this post at ten kilometers altitude on a flight home from Mexico, Blink Shell open on my iPhone, while an agentic assistant on my NixOS machine writes an initial draft for this post.
Admittedly, Iâ€™m a little addicted to agentic coding: when an idea pops up, I want to jump on it right awayâ€”even if that means doing it from a cramped seat in the sky.
This is the workflow Iâ€™ve arrived at after trying many alternatives, it extends [my agentic coding write-up]({{< ref "/post/agentic-coding" >}}) and the self-hosted AI obsession in [my local AI journey]({{< ref "/post/local-ai-journey" >}}).
It's my backup plan for those moments when a computer simply isn't nearbyâ€”the rest of the time I'm still at a keyboard like any other developer.
This is in no way a replacement for a proper computer/laptop workflow but gets the job done when needed.
Itâ€™s a personal, mostly openâ€‘source stack: `agent-cli` is my localâ€‘first voice and clipboard layerâ€”streaming mic audio to FasterWhisper for ASR, cleaning/polishing with a local LLM via Ollama, and (when needed) speaking results via Piper.
It also exposes a small HTTP interface that my iOS Shortcut hits to send audio and receive cleaned text.
The coding agent still calls into the best proprietary frontier model I can access.

{{% callout note %}}
Meta: Iâ€™m writing about agentic coding on mobile from my actual phone.
Itâ€™s not the most efficient way to write, but it gets the job done â€” during a 6.5â€‘hour flight I dictated and iterated this entire post, although I took more than 50 iterations (>50 commits) to get it right.
See [PR #40](https://github.com/basnijholt/nijho.lt/pull/40) for the full review trail.
{{% /callout %}}

{{% callout note %}}
**TL;DR:** iPhone â†’ WireGuard â†’ Blink+Mosh â†’ Zellij.
I dictate via a Shortcut â†’ FasterWhisper (transcribe) â†’ Ollama (polish) â†’ clipboard, then paste into Codex CLI using OpenAIâ€™s `gpt-5-codex-high`.
Everything is local except the model.
{{% /callout %}}

{{< toc >}}

## 1. Why Phone Coding Works Now

For years I used **iSH** (full Alpine Linux emulator) with an SSH client on my phone to hop into servers.
Because coding on a phone keyboard is terrible, I kept it to tiny configuration tweaks or one-off fixes of a few characters.
Agentic tools changed that: with a CLI coding agent, I donâ€™t need to type the codeâ€”I describe the change, review the patch, and run it.
That made meaningful work on the phone possible for the first time, for those moments when a computer isnâ€™t around.

I tried **VS Code in the browser**, bounced between **iSH** and **Terminus** for SSH, and even lived inside a handful of in-browser terminal clients.
I also spent time with mobile companions like [**Happy**](https://happy.engineering/) and [**Omnara**](https://www.ycombinator.com/companies/omnara), both designed to mirror Claude Code sessions on the phone, but they still felt like another relay layer between me and my shell.
That friction pushed me toward a phoneâ€‘ready, selfâ€‘hosted workflow that still gives me raw SSH access to my own machine when the computer is out of reach.

This post is based on the way I develop software today.
Your mileage may vary, but if you also care about privacy, open tooling, and reproducible environments, I think there are useful pieces here.

## 2. Constraints and Trade-offs

This started with a few constraints and grew organically; today these are my guiding principles:

- **Single trust boundary:** Only the model provider (OpenAI) sees code context; audio and automation stay local, and I avoid any additional thirdâ€‘party relays.
- **Resilient sessions:** Connections should survive sleep and spotty networks.
- **Voice-friendly:** Dictation should be accurate enough that I can trust it.
- **Reproducible config:** The entire stack must live in my [dotfiles]({{< ref "/post/dotfiles" >}}) and [NixOS configuration](https://github.com/basnijholt/dotfiles/tree/8f6bf0b7219195a46a3e010d3538e1e449634db7/configs/nixos).

To give you a sense of what I tried, here is the short comparison that convinced me to roll my own:

| Approach | Pros | Why I moved on |
| --- | --- | --- |
| VS Code in the browser | Familiar editor UI | Needs a steady connection and still lives outside my dotfiles comfort zone |
| iSH / Terminus SSH | Works without extra infrastructure | Laggy, no Mosh, and awkward keybindings |
| In-browser terminals | Instant access from anywhere | Poor copy/paste ergonomics and flaky mobile keyboards |
| [Happy](https://apps.apple.com/us/app/happy-codex-claude-code-app/id6748571505) (Claude Code companion) | Push notifications, encrypted mobile UI for Claude Code | Requires wrapping every session with a separate CLI and still abstracts away my shell |
| [Omnara](https://omnara.com) (agent command center) | Centralizes Claude Code/Codex sessions with terminal replay | Proxies via their servers; I already trust OpenAI for the model and don't want another thirdâ€‘party handling my code |

The stack below gives me the resilience of Mosh, the ergonomics of Zellij, and full control over the AI layer.

## 3. Layer 1: WireGuard From the Router

I terminate WireGuard on my router so every device in the house (and on the road) can dial home with the same config.

- **Server:** ASUS XT8 router with WireGuard enabled via the router UI.
- **Client:** The WireGuard iOS app with `On-Demand` rules so the tunnel flips on whenever I am off trusted Wi-Fi.
- **DNS:** All mobile sessions resolve through my home DNS, so `git.nijho.lt` and internal services resolve instantly.

This gives Blink a local-LAN address for my desktop `nixos`, without hairpin NAT or double SSH jumps.

## 4. Layer 2: Blink Shell + Mosh for Durable Sessions

What is [Mosh](https://mosh.org/)?
It's like SSH, but it stays connected when the network changes or the phone sleeps, and it feels faster on bad connections.

Blink Shell is my daily driver on iOS because it pairs beautifully with Mosh and has solid keyboard ergonomics (external keyboards, sane modifiers, and reliable shortcuts).

- I launch sessions using `mosh bas@nixos -- zellij attach -c phone`.
- Mosh smooths over spotty LTE and keeps my session alive when the phone sleeps.
 
Mosh keeps the session responsive across flaky networks and sleep.

## 5. Layer 3: Zellij Layouts

I use **Zellij** as my terminal multiplexer for mobile work.

- I stick to the defaults and use a couple of predefined layouts (e.g., a "phone" layout with editor + shell panes).

Ergonomics matter even more on a glass keyboard, so I lean on a few shell helpers:

- **[`zoxide`](https://github.com/ajeetdsouza/zoxide)** means I can jump between repos with `z foo` instead of pecking long paths.
- **Single-character aliases** like `p` for `pytest` and `gs` for `git status` live in [`configs/shell/10_aliases.sh`](https://github.com/basnijholt/dotfiles/blob/8f6bf0b7219195a46a3e010d3538e1e449634db7/configs/shell/10_aliases.sh), keeping command entry to a minimum.
- **Autocompletion plugins**â€”[`zsh-autosuggestions`](https://github.com/zsh-users/zsh-autosuggestions) and `zsh-syntax-highlighting`â€”are wired up in [`configs/shell/70_zsh_plugins.sh`](https://github.com/basnijholt/dotfiles/blob/8f6bf0b7219195a46a3e010d3538e1e449634db7/configs/shell/70_zsh_plugins.sh), so I get inline hints and colour cues while typing.

I break these tricks down in more depth in [Terminal Ninja]({{< ref "/post/terminal-ninja" >}}), explain how I sync them with my [Dotfiles]({{< ref "/post/dotfiles" >}}), and even package binaries like `zoxide` with [Dotbins]({{< ref "/post/dotbins" >}}).

## 6. Layer 4: `agent-cli` Server

[`agent-cli`](https://github.com/basnijholt/agent-cli) runs as a small server on my NixOS machine.
In this workflow I only use `transcribe` from iOS Shortcuts: the phone records audio, the server transcribes and cleans it up, and I paste the text.
Agentâ€‘CLI also provides commands like `autocorrect`, `voice-edit`, a wakeâ€‘word `assistant`, and a conversational `chat` agent, but Iâ€™m not using those here.
On this machine I run a longâ€‘lived `agent-cli` user service (systemd) so Shortcuts can POST audio to it and get cleaned text backâ€”ready to paste into Codex CLI.
The service is defined in my dotfilesâ€”see: [`configs/nixos/modules/user.nix` (agent-cli service)](https://github.com/basnijholt/dotfiles/blob/8f6bf0b7219195a46a3e010d3538e1e449634db7/configs/nixos/modules/user.nix#L29-L40).

For more background on why I built it and how I use it in practice, see [Local AI Journey]({{< ref "/post/local-ai-journey" >}}) (Â§3) and [Agentic Coding]({{< ref "/post/agentic-coding" >}}).

The models and services run on the same box:

- **FasterWhisper** via [`faster-whisper-server`](https://github.com/SYSTRAN/faster-whisper) for highâ€‘accuracy streaming transcription.
- **Ollama** for onâ€‘device rewrite/cleanup before sending prompts to the coding agent.

My iOS Shortcut record roundtrip is slower than Appleâ€™s onâ€‘device dictation, but the accuracy is much better, which matters more.

For the actual coding agent, I use a [fork](https://github.com/just-every/code) of Codex CLI with OpenAIâ€™s `gpt-5-codex-high` model (here no open-source solution matches the frontier).

## 7. Layer 5: The iOS Shortcut Pipeline

My iPhone's Action Button runs a Shortcut that records audio and sends my voice to agent-cli on my NixOS machine for transcription and cleanup.

For the full recipe, see the iOS Shortcut Guide: [agent-cli/iOS_Shortcut_Guide.md](https://github.com/basnijholt/agent-cli/blob/main/iOS_Shortcut_Guide.md).

In short: I press the Action Button, the Shortcut records a snippet, sends it to `agent-cli`, and copies the cleaned text to my clipboard so I can paste it into Codex CLI.

The whole loop finishes fast enough that I can capture intent by voice and paste it into Codex CLI without typing.

{{% callout note %}}
**Dictation quality:** In my experience, the builtâ€‘in iOS dictation is absolute garbage compared to Whisper/FasterWhisper.
Accuracy and punctuation are much better with the FasterWhisper server running at home; itâ€™s a bit slower, but that tradeâ€‘off is worth it for clean, usable text.
{{% /callout %}}

## 8. Workflow in Practice

- Ensure WireGuard is connected.
- Open Blink and the session is already open! (To initialize, I Mosh into `nixos`; `zellij` reattaches with my existing panes.)
- Dictate the feature/change, paste into Codex CLI (using `gpt-5-codex-high`), and iterate.
- Review diffs either in another Mosh/Zellij pane or as a pull request.
- An agent commits and pushes to a branch, then opens a PR via `gh` for me to review and merge.

On the phone, I aim for the smallest practical edits and initial implementations.
Even on personal repos, I still open a PRâ€”sometimes prompting alone gets me to a mergeâ€‘ready result.
For openâ€‘source with real users, I finish on the computer with a careful review and any final edits.

## 9. Conclusion and Further Reading

This phone setup simply extends the workflow from [Agentic Coding]({{< ref "/post/agentic-coding" >}}).
I connect to the same Zellij session on `nixos`, so when a new idea hits, I can pick up exactly where I left off and keep goingâ€”no new environment, no copyâ€‘paste dance.
Everything stays on my hardware (voice and automation), with only the coding model (`gpt-5-codex-high`) living behind an external API.
Itâ€™s the most effective mobile workflow Iâ€™ve had so far.
Itâ€™s mostly openâ€‘source not out of dogma, but because those tools are the best options for my needs.
The one exception is the coding model: thereâ€™s no true open equivalent right now, and it makes no sense to buy 20Ã— H100s just to selfâ€‘host a frontier model even if such a high quality model would be available open-source.

If you're curious to dive deeper, here are a few related posts:

- [Agentic Coding]({{< ref "/post/agentic-coding" >}})
- [Dotfiles: A Practical, Cross-Platform Terminal Setup]({{< ref "/post/dotfiles" >}})
- [Local AI Journey: RTX 3090 Edition]({{< ref "/post/local-ai-journey" >}})
- [Terminal Ninja]({{< ref "/post/terminal-ninja" >}})
- [Using LLMs Effectively]({{< ref "/post/using-llms" >}}) (already outdated at this point!)

I'd love to hear what self-hosted tricks you're using for mobile developmentâ€”reach out if you adapt this stack or build something wild on top of it.
